\documentclass[a4paper]{llncs}

\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc} 
\input{utf8symbols}
\usepackage{stmaryrd}
\usepackage{amsmath,amssymb}
\usepackage{mathpartir}
\usepackage{url}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}

\newcommand{\wpre}{{\bf wp}}
\newcommand{\sep}{ ~|~ }
\newcommand{\letml}{{\bf let}}
\newcommand{\inml}{{\bf in}}
\newcommand{\ifml}{{\bf if}}
\newcommand{\thenml}{{\bf then}}
\newcommand{\elseml}{{\bf else}}
\newcommand{\refml}{{\bf ref}}
\newcommand{\preml}{{\bf pre}}
\newcommand{\postml}{{\bf post}}
\newcommand{\propml}{{\bf prop}}
\newcommand{\boolml}{{\bf bool}}
\newcommand{\recml}{{\bf rec}}
\newcommand{\efft}[1]{\langle #1 \rangle}
\newcommand{\lift}[1]{\langle #1 \rangle}
\newcommand{\alist}[1]{\overline{#1} }
\newcommand{\Trueml}{{\bf True}}
\newcommand{\Falseml}{{\bf False}}
\newcommand{\trueml}{{\bf true}}
\newcommand{\falseml}{{\bf false}}
\newcommand{\unitml}{{\bf unit}}
\newcommand{\intml}{{\bf int}}
\newcommand{\coq}{Coq}
\newcommand{\who}{Who}
\newcommand{\ocaml}{Ocaml}
\newcommand{\correct}[1]{c(#1)}

\newcommand{\ceil}[1]{\lceil #1 \rceil}

\begin{document}
\title{\mbox{Verifying Imperative Higher-Order Programs}}

\author{Johannes Kanig\inst{1,}\inst{2} \and Jean-Christophe
Fill\^atre\inst{2,}\inst{1}}
\institute{
INRIA Saclay - Île-de-France, ProVal, Orsay, F-91893 \and
 LRI, Univ Paris-Sud, CNRS, Orsay, F-91405
}
\maketitle

\begin{abstract} We introduce a specification language suited for reasoning
  about effectful higher-order programs. The language's main features are
  higher-order logic and effect polymorphism, both of which enable a truly
  modular treatment of higher-order functions.  We also define a weakest
  precondition calculus for this language which generates proof obligations in
  higher-order logic.  To our knowledge, this is the first time that effect
  polymorphism is put to use in program verification. Additionally, we propose
  a region-based analysis to exclude aliasing, in order to obtain much simpler
  proof obligations. Several examples demonstrate the practicality of the
  system. A prototype implementation exists and has been used to prove the
  correctness of these and more examples.  
\end{abstract}

\section{Introduction}

The purpose of the present paper is to propose a language suited for reasoning
about effectful higher-order programs.  Higher-order functions are a key
feature, not only of purely functional languages like Haskell and the pure
fragment of ML, but also in combination with imperative features, like
references or arrays. However, reasoning about higher-order functions is
difficult, and especially so in the presence of effectful computations. Any
function's specification depends on its arguments. In a higher-order setting,
arguments may be functions that have their own specification. As a consequence,
to be modular, the specification of a higher-order function can only be
formulated {\em depending on the specifications of its functional arguments}.
In a setting where side effects are possible, one also needs to formulate the
specification {\em depending on the side effects of the functional arguments}.
We claim that to be truly modular, the specification of a higher-order
function must be generic with respect to the specification {\em and} the
effect of its arguments.

In this paper, we first present a specification language combining features of
a programming language and logic. It uses an effect system and a region
calculus to track references. The effect system contains {\em effect
polymorphism}, thus dealing well with higher-order functions. The
specifications can be expressed in higher-order logic, to be able to state
properties of functional arguments.  We then proceed to define a weakest
precondition calculus for this language, transforming an annotated
program into a logic formula, which has to be valid for the program to be
correct. This calculus is very straight-forward and syntax-directed.

Programs in our language may contain aliases. However, we obtain much simpler
proof obligations when aliases are excluded. Therefore, we also propose a
static region-based analysis rejecting programs with aliasing. The weakest
precondition calculus on the alias-free fragment has been implemented in a
prototype tool called \who~\cite{KanigFilliatre09wml}.  We show the usefulness
of this tool on several examples, which have all been proved correct. 

\paragraph{Outline.}
Section~\ref{sec:genpic} introduces our language using a motivating example.
Sections~\ref{sec:syntax} and \ref{sec:wp} give the details about the
programming language and the weakest precondition calculus.
Section~\ref{sec:alias} shows how two modifications to the type system can
eliminate aliasing. In section~\ref{sec:examples} we present our
implementation and some examples. Finally, section~\ref{sec:related} discusses
related work.

\section{The General Picture and a Motivating Example}
\label{sec:genpic}

Consider the following first-order program\footnote{Throughout the document,
we use \ocaml~syntax for programs.} that accumulates the sum of the contents
of some array {\tt a} in some global reference {\tt c}, using an integer {\tt
i} as a counter:
\begin{ocaml}
  let sum () = 
    for i = 0 to Array.length a - 1 do
      c := !c + a.(i)
    done
\end{ocaml}
This simple program can be annotated in a standard Hoare logic (for
example the system of \cite{Filliatre00a}) as follows:
\begin{who}
  let sum () =
    { !c = 0 }
    for i = 0 to Array.length a - 1 do
      { !c = {{{ $\sum_0^{i-1}$ }}}  a.(i) }
      c := !c + a.(i)
    done
    { !c = {{{ $\sum_0^{ {\tt Array.length~a} - 1}$ }}}  a.(i) }
\end{who}
Our program is a {\tt for} loop, so it requires an {\em invariant
annotation}, indicating which relation is preserved by the loop body. The
equalities that need to be proved are very simple. Also, it is obvious that
the program only modifies the reference {\tt c}, and this fact can be
formally confirmed by an effect analysis.

A common reaction of a functional programmer to a program like
{\tt sum} is to factor out the structure of the recursion, which
may be useful in other situations, in a higher-order function. In this case,
the more general function is the following iterator over arrays:
\begin{ocaml}
  let iter f a = 
    for i = 0 to Array.length a - 1 do
      f a.(i)
    done
\end{ocaml}
The more specific program is then recovered by function application. 
\begin{ocaml}
  let sum () =
    iter (fun n -> c := !c + n) a
\end{ocaml}
It is obvious that the two programs do exactly the same
thing. However, how do we prove the correctness of the second one? One
possibility is to impose the future usage of ${\tt iter}$ by
specifying a particular specification for its argument {\tt f}:
\begin{who}
  let iter 
    (f : (x : int) ->{c} 
        { !c = {{{ $\sum_0^{i-1}$ }}} a.(i) } 
        unit 
        { !c = {{{ $\sum_0^{ {\tt Array.length~a} - 1}$ }}} a.(i) } ) 
    (a : int array) =
    ...
\end{who}
Here we impose precisely the specification of {\tt f}, and thus we can
derive that {\tt sum} computes the expected value,
using the same method as for the first-order case before. Note
that, to be able to apply an effect analysis, we have to specify that
$f$ modifies $c$, by giving it the type $\intml->^{\{c\}}\unitml$.

Of course, this approach is not very satisfactory. While the program achieves
the separation of the iterating function ({\tt iter}) and the combining
function (the anonymous function), the specification does not do this at all;
as specified,  {\tt iter} can basically only be applied to the same function,
and can only compute a certain sum. If one wants to use {\tt iter} in another
context, one has to copy the code, give it a new specification adapted to the
application and prove this new specification again. In addition, in order to
be able to write the specification at all, we had to specialize the type of
{\tt iter} to integer arrays, while its \ocaml\ counterpart is of course
polymorphic. Said otherwise, this approach is not modular at all.

There are two obstacles related to higher-order functions to be overcome in
order to achieve modularity on the level of specification in the presence of
side effects. We must be able to refer to the effect of a function parameter,
and we must be able to refer to its specification (pre- and postconditions),
in a generic way.

The first obstacle has a well known solution using a type and effect
system~\cite{talpin94taed} with effect variables: effect polymorphism does
exactly what we want here. Using effect polymorphism, we can say that {\tt f}
(the argument of {\tt iter}) has type $α->^e\unitml$, where $α$ is the type of
the array elements and where $e$ is an {\em effect variable}, which amounts to
say that {\tt f} has a {\em certain} effect $e$. The complete type of {\tt
iter} is thus:
\begin{displaymath}
  ∀α∀e. (α ->^e \unitml) ->^\emptyset α~\mathtt{array} ->^e \unitml
\end{displaymath}
The total effect of {\tt iter} is then $e$, meaning {\tt iter}
modifies whatever {\tt f} modifies.

A solution to the second problem concerning specifications consists in
representing a program function in the logic as the pair of its pre- and
postcondition. This is an old idea, and it has recently been applied to purely
functional programs by Régis-Gianas and
Pottier~\cite{regis-gianas-pottier-08}. 

The main idea of this paper is to combine the effect-based Hoare logic of
\cite{Filliatre00a} with the more powerful effect system of
\cite{talpin94taed} and the more powerful annotations of
\cite{regis-gianas-pottier-08} to obtain a modular system for proving
higher-order programs with side effects. As in~\cite{regis-gianas-pottier-08},
we use operators \preml\ and \postml\ to access the pre- and postcondition of
a function. However, in our system, a precondition takes as an argument not
only the function argument, but also the initial state, before the execution
of the function. The postcondition takes the argument and return value, but
also the states before {\em and} after the function call. Two-state
postconditions are a standard way to get rid of most auxiliary variables.
However, the pre- and postcondition of {\tt f} may not refer to arbitrary
parts of the state, but only the part {\tt f} reads or modifies.  With these
conventions, one can now write a generic implementation and specification for
the {\tt iter} function. It is given~Fig.~\ref{fig:iter}, in the input syntax
of our prototype \who. There are a few syntactic differences with \ocaml, in
particular for arrays: the type of arrays is {\tt 'a farray ref}, the length
of the array {\tt a} is {\tt length !a} in the program and {\tt length !!a} in
the logic, and similarly access to the {\tt i}th element is {\tt get i !a} in
the program and {\tt get i !!a} in the logic.

\begin{figure}[tbp]
\begin{who}[lines]
let iter [e] (a : 'a farray ref) (inv : <e> -> int ->  prop)
  (f : 'a ->{e} unit) =
  { inv cur 0 /\ forall (i:int). 0 <= i /\ i < length !!a ->
    forall (m : <e>).  inv m i -> pre f (get i !!a) m /\
    forall (n : <e>).  post f (get i !!a) m n () -> inv n (i+1)
  }
  for index = 0 to length !a - 1 do
    { inv cur index }
    f (get index !a)
  done
  {inv cur (length !!a) }
\end{who}
\caption{The specification of the {\tt iter} function}
  \label{fig:iter}
\end{figure}

There are several new elements here. First of all, we use effect polymorphism.
An effect variable {\tt e}, standing for the effect of {\tt f}, is introduced
by the notation ${\mathtt [e]}$.  {\tt iter} also has an additional argument
{\tt inv}, the {\em invariant}, taking as argument the portion of the state
modified by {\tt f} and the integer indicating the current iteration count.
This is a logical argument, which means that it cannot be used in the program.
Also, {\tt inv} is a {\em pure} function, which is indicated by the arrows
without effect annotation. The postcondition is now quite clear: we simply
want to establish the invariant for the final state (the keyword {\tt cur}
stands for the current state of the postcondition) and the length of the
array, indicating that we have iterated over the entire array. It is also easy
to see that the invariant {\tt inv} is precisely the invariant of the {\tt
for} loop.  However, the precondition is a bit complicated: it states that we
need the invariant to hold initially (for the current state of the
precondition and index $0$) and we need the invariant to be preserved by the
function call.  This is expressed by quantifying over two states, saying that
the invariant has to imply the precondition of {\tt f} and the postcondition
has to imply the invariant for the next array index.  With a bit of syntactic
sugar, this could be expressed much more concisely using a Hoare triple:
\begin{who} 
  { inv cur i } f (get i !!a) { inv cur (i+1) } 
\end{who}

The specification of {\tt iter} is correct, but it will be difficult to prove
in general, because it is not clear whether {\tt f} modifies the array or not,
or more precisely, if the effect variable {\tt e} does contain {\tt a} or not.
This is an instance of the {\em aliasing problem} (see, for
example, \cite{reynolds78popl}) and it is well known that standard Hoare logic
is not well-suited for programs with aliasing. Without special techniques (for
example {\em separation logic}), a large number of equalities and disequalities
are needed to reason about potentially aliased references. 

That is why we also propose in this paper a simple technique to exclude
programs with aliasing and to obtain simpler proof obligations. The solution
consists in restricting the instantiation of effect variables. In the example
of Fig.~\ref{fig:iter}, this means that {\tt e} could not be instantiated with
an effect containing {\tt a}.  With aliasing exclusion enabled, our system is
of course inappropriate for reasoning about shared mutable structures.
However, it is interesting to see that this alias exclusion restriction, in
many cases, corresponds to a natural condition imposed on higher-order
functions that operate on some data structure. It is a common
requirement~\cite{Nanevski08Awkward,Krishnaswami06} for iteration functions,
for example, that the functional argument must not modify the structure to be
iterated on. In our system, this fact follows from the restriction on effect
instantiations, as will be seen in section~\ref{sec:alias}. 

It is important to note that neither our specification language nor its
weakest precondition calculus are restricted to alias-free programs. The
aliasing exclusion is only a means to get simpler proof obligations whenever
possible.

\section{A Programming Language for Specification}
\label{sec:syntax}

\subsection{Syntax}
\begin{figure}[tpb]
\begin{eqnarray*}
  x & & \text{Program Variables}\\
  α & & \text{Type Variables}\\
  e & & \text{Effect Variables}\\
  ρ & & \text{Region Variables}\\
  χ &::=& α \sep e \sep ρ \\
  t,p,q &::=& x~[\alist{κ}] \sep λx:τ.t \sep \recml~f~(x:τ).\{p\}t\{q\} \sep t~t\sep \\ 
  & & \letml~ x~[\alist{χ}] = t~\inml~t \sep \ifml~t~\thenml~t~\elseml~t\\
  τ &::=& α \sep τ -> τ \sep τ ->^ε τ \sep τ \times τ \sep τ~\refml_ρ \sep
  \efft{ε} \\
  ε &::=& (ρ \sep e)*\\
  κ &::=& τ \sep ε \sep ρ
\end{eqnarray*}
  \caption{Syntax}
  \label{fig:syntax}
\end{figure}

Our programming language (Fig.~\ref{fig:syntax}) is basically the λ-calculus
with \letml-bindings, where there are two different λ-bindings. The first one
($λx:τ.t$) describes a pure, non-recursive binding, and the second one
($\recml~f~(x:τ).\{p\}t\{q\}$) describes recursive, potentially impure
functions; $p$ and $q$ in this term are {\em specifications}, which are
of the same syntax as programs, but are differently typed (typing will be
discussed in the next subsection). The term $p$ corresponds to the {\em
precondition}, $q$ to the {\em postcondition} of the function. The analogy to
Hoare triples is obvious. As usual, there are also variables,
\letml-bindings and an \ifml-\thenml-\elseml-construct. Variables on the type
level can be generalized at \letml-boundaries and have to be instantiated when
the corresponding program variable is used.

Before we describe the syntax of types, let us first explain {\em regions} and
{\em effects}. Regions are simply type level names for memory locations. An
effect is a set of memory locations and {\em effect variables}. An effect
variable stands for an unknown effect. Effects are used to specify
the memory locations an expression can write. They are also used in function
types to describe the memory locations which may be written if the function is
called.

Type, region and effect variables must be explicitly generalized and
instantiated. For this purpose, we have defined the metavariable $χ$,
representing variables, and $κ$, representing the corresponding instantiation
objects types, regions and effects\footnote{We do not distinguish between
region variables and region constants; a region constant is simply a region
variable whose scope is the entire program.}. Now, it can be seen that
generalization takes place at \letml-bindings and instantiation at variable
occurrences. We use a horizontal bar over terms (like in ${\overline{χ}}$) to
denote lists.

\subsection{Typing}

We have already seen that programs and specification share in fact the same
syntax. Though they are differently typed, their typing relations still share
a lot. For this reason, and to underline the fact that in Hoare logic, values
are shared between programs and the logic, we present three different typing
relations. One for programs, one for specifications, and the common part is
factored out in a third typing relation for values. All typing relations
contain an environment $Γ$, which is simply a map from variable names to {\em
type schemes} of the form $∀\alist{χ}.τ$ .

\begin{figure}[tbp]
  \framebox{$Γ|-_v t : τ$}
  \begin{mathpar}
  { \inferrule*[Left=Var] {Γ(x) = ∀\alist{χ}.τ} {Γ|-_v x~[\alist{κ}] :
  τ[\alist{χ}|->\alist{κ}]} } \and
  { \inferrule*[Left=PureFun] {Γ,x : τ' |-_v t : τ} {Γ|-_v λx : τ'.t : τ' ->
  τ} } \and
  { \inferrule*[Left=Rec] 
  { Γ' = Γ, x : τ' \\ Γ'' = Γ',~f : τ' ->^ε τ \\ Γ'' |- t : τ,ε \\ Γ' |-_l
  p : \efft{ε} -> \propml \\ Γ' |-_l q : \efft{ε} ->
  \efft{ε} -> τ -> \propml } 
      {Γ|-_v \recml~f~(x : τ').\{p\}t\{q\} : τ' ->^ε τ} 
  } \and
  { \inferrule*[Left=PureApp]
    {Γ |-_v t1 : τ' -> τ \\ Γ|-_v t2 : τ'}
    {Γ |-_v  t1 ~ t2 : τ}
  }
  \end{mathpar}
  \caption{Value typing}
  \label{fig:valuetyping}
\end{figure}

\paragraph{Value typing.}
Let us first look at the common part concerning values. We define a relation
$Γ|-_v t : τ$, which means ``$t$ is well-typed wrt. the environment $Γ$, and
is of type $τ$''. Values cannot have any effect.  Variables and pure
abstractions are typed in the usual way; type instantiations at variable
occurrences are substituted in the type. The rule {\sc REC} simply states that
the body of the function can use the argument $x$ as well as the function $f$.
The specifications $p$ and $q$, however, cannot use $f$ and must be well-typed
wrt. the typing relation $|-_l$ described later on. Finally, pure applications
between values are considered to be values as well, as they don't have any
effect.

\begin{figure}[tpb]
  \framebox{$Γ|- t : τ,ε$}
\begin{mathpar}
  { \inferrule*[Left=Value]
  { Γ |-_v t : τ  }
  { Γ |- t : τ,\emptyset  }
  } \and
  { \inferrule*[Left=App]
    {Γ |-_v t1 : τ' ->^ε τ \\ Γ|-_v t2 : τ'}
    {Γ |- t1 ~ t2 : τ, ε}
  } \and
  { \inferrule*[Left=ite]
    { Γ |- _v t1 : \boolml \\ Γ |- t2 : τ, ε1 \\ Γ |- t3 : τ, ε2 }
    { Γ |- \ifml~ t1 ~\thenml~ t2 ~\elseml ~ t3 : τ, ε1  ε2 }
  } \and
  { \inferrule*[Left=Letv]
    { Γ, \alist{χ} |-_v t1 : τ' \\ Γ, x : ∀\alist{χ}.τ' |- t2 : τ,ε }
    { Γ |- \letml~ x~[\alist{χ}] = t1 ~ \inml~ t2 : τ, ε}
  } \and
  { \inferrule*[Left=Let]
    { Γ |- t1 : τ', ε \\ Γ, x : τ' |- t2 : τ, ε }
    { Γ |- \letml~ x = t1 ~ \inml~ t2 : τ, ε }
  } \and
  { \inferrule*[Left=Sub]
    { Γ |- t : τ', ε \\ ε \subseteq ε' }
    { Γ |- t : τ, ε' }
  }
\end{mathpar}
  \caption{Program typing}
  \label{fig:progtyping}
\end{figure}

\paragraph{Program typing.}
The typing of programs, written $Γ|- t : τ,ε$, assigns a type $τ$ and an
effect $ε$ to an expression $t$, given an environment. This typing is
presented in {\em A-normal form}, a normal form where all intermediate values
are \letml-introduced. This is not a restriction, as any program can be
transformed into A-normal form.

All values are also programs with no effect (rule {\sc
Value}). An application must consist of two values and the effect of the
overall expression is precisely the effect of the called function (rule {\sc
App}). There are two rules for \letml, due to the value restriction. One only
can generalize type, effect and region variables when the bound expression is
a value. In this case (rule {\sc LetV}), only the second term may produce
effects. In the other case (rule {\sc Let}), no generalization takes place,
but both terms can have effects. The {\sc Let} rule actually requires both
terms to have the same effect, but together with the {\sc Sub} rule, which can
increase the effect of an expression, this can always be achieved.
Finally, \ifml-expressions are typed in the usual way, both branches have to
have the same effect.

\paragraph{Logic typing.}

The typing of annotations is basically an extension of the typing for values;
logic terms never have any effect. To ensure this, we need to forbid calling
effectful functions in the logic. One way is to simply omit the typing rule
for effectful function calls. There is, however, another way which also
enables us to inspect the pre- and postcondition of a function. To this end,
let us introduce the operation of {\em lifting} a type to the logic level (see
Fig.~\ref{fig:typelift}).
\begin{figure}[tpb]
  \begin{eqnarray*}
    \ceil{α} &=& α\\
    \ceil{τ~\refml_ρ} &=& \ceil{τ}~\refml_ρ\\
    \ceil{τ -> τ'} &=& \ceil{τ} -> \ceil{τ'}\\
    \ceil{τ \times τ'} &=& \ceil{τ} \times \ceil{τ'}\\
    \ceil{\efft{ε}} &=& \efft{ε}\\
    \ceil{τ ->^ε τ'} &=& (\ceil{τ} -> \efft{ε} -> \propml)\times (\ceil{τ} ->
    \efft{ε} -> \efft{ε} -> \ceil{τ'} -> \propml)
  \end{eqnarray*}
  \caption{Lifting types}
  \label{fig:typelift}
\end{figure}
This translation traverses the type and replaces all effectful types by a
tuple type which describes a tuple of a pre- and a postcondition. The
precondition takes as an argument the argument of the function as well as the
part of the store, at the moment of the function call, which corresponds to
the effect of the function. The postcondition's two first arguments are the
same, but in addition it expects the store {\em at the exit of the function},
as well as the function's return value.

\begin{figure}[tbp]
  \framebox{$Γ|-_l t : τ$}
  \begin{mathpar}
  { \inferrule*[Left=Logic]
    { Γ |-_v t : τ  }
    { Γ |-_l t : \ceil{τ} }
  } \qquad \and
  { \inferrule*[Left=LetLogic]
    { Γ, \alist{χ} |-_l : t' : τ' \\ Γ, x: ∀\alist{χ}.τ' |-_l t : τ  }
    { Γ |-_l \letml~x~[\alist{χ}] = t'~\inml~t : τ }
  }\and
  { \inferrule*[Left=IteLogic]
    { Γ |- _l t1 : \boolml \\ Γ |-_l t2 : τ \\ Γ |-_l t3 : τ }
    { Γ |- \ifml~ t1 ~\thenml~ t2 ~\elseml ~ t3 : τ }
  }
  \end{mathpar}
  \caption{Logic typing}
  \label{fig:logictyping}
\end{figure}

Now we can come back to logic typing. Every value is also a logic term,
but now it is assigned its {\em lifted} type. The two other rules are just 
pure variants of their impure counterparts in the program typing.

\paragraph{Predefined constants.}

To actually be able to write programs in this language, we need many
predefined constants and types. The typing relations already mention two,
\propml\ and \boolml. \propml\ is the logical truth and contains the values
\Trueml\ and \Falseml, while \boolml\ describes boolean (decidable) properties
and contains \trueml\ and \falseml. The distinction between \propml\ and
\boolml\ is the same one as in \coq, for example. Properties expressed in
\propml\ are potentially undecidable and one cannot take decisions by testing
their truth value. On the other hand, \boolml\ talks about decidable
properties and so it makes sense to expect the predicate of the
\ifml-statement to be of type \boolml.

The reader may have wondered why the specification language does not contain
any logical connectives. These can of course be declared independently, for
example the conjunction is just a function: 
\begin{equation*} 
  /\ :  \propml -> \propml -> \propml 
\end{equation*} 
Actually, when equality is given, all logical operations (even quantification)
can be defined in the logic~\cite{andrews86}.  In the remainder of the paper,
we suppose predefined the usual logical connectives conjunction, disjunction,
implication and equivalence, as well as universal and existential
quantification. We also assume all the arithmetic operations as well as the
constructor $(t,t)$  and the accessors $\mathit{fst}$ and $\mathit{snd}$ for
pairs. We will also use the names \preml\ and \postml\ for $\mathit{fst}$ and
$\mathit{snd}$, respectively.

The store types $\efft{ε}$ describe a portion of the store and are basically
mappings from region variables to values. The domain of the mapping is the
effect $ε$, where effect variables stand for unknown parts of the domain. To
deal with objects of these types, we introduce three symbols
\begin{eqnarray*}
  \oplus &:&∀ e1 e2 . \efft{ e1 } -> \efft{ e2 } -> \efft { e1  e2 }\\
  |_{ε} &:& ∀e. \efft{ e ε } -> \efft{ ε }\\
  {\tt !!} & : & ∀αρe. α~\refml_ρ -> \efft{ ρ e } -> α.
\end{eqnarray*}
The first one denotes a combining operation for stores; the resulting store
contains all the mappings of the second store, as well as all the mappings of
the first one, as long as they are not present in the second one. The second
operations is indexed by an effect and it restricts a store to a certain
effect. Finally, the double bang {\tt !!} is the logic equivalent to the
lookup operator {\tt !}. It is pure and it is always with respect to a certain
state.

\subsection{References}

We haven't mentioned references yet, but all we need to support them is to add
the three usual functions $\mathit{ref}, :=, !$ for reference creation, assignment and
lookup, respectively, to the environment. In our system, these functions have
the following types:
\begin{eqnarray*}
  \mathit{ref} &:& ∀αρ.α ->^ρ α~\refml_ρ\\
  := &:& ∀αρ.α~\refml_ρ ->^\emptyset α ->^ρ \unitml\\
  ! &:& ∀αρ. α~\refml_ρ ->^ρ α
\end{eqnarray*}

\section{A Weakest Preconditions Calculus}
\label{sec:wp}

The aim of the weakest precondition calculus is to obtain proof obligations
from annotated programs. To do this, the calculus starts from a formula
which holds, or is supposed to hold, at the end of the program, and walks
backwards in the program source to obtain the weakest formula which has to
hold at the beginning of the program (hence the name {\em weakest
precondition}). 

In the following, we use the metasymbol $v$ to denote well-typed values, i.e.
terms with a typing derivation $Γ|-_v v : τ$. In the definition of the weakest
precondition calculus, we will use the notation $(t : τ, ε)$ to denote that
$t$ has type $τ$ and effect $ε$. We assume that applications of the
subeffecting rule {\sc Sub} rule are explicitly inserted in the term; they
are written $(t:ε<:ε')$.

\begin{figure}[tbp]
  \begin{eqnarray*}
    \ceil{x~[\alist{κ}]} &=& x~[\alist{κ}]\\
    \ceil{λx:τ.t} &=& λx:τ.\ceil{t}\\
    \ceil{\recml~f~(x:τ).\{p\} t \{q\}} &=& 
    (λx:\ceil{τ}.p,λx:\ceil{τ}.q)\\
    \ceil{t_{τ' -> τ}~t'} &=& \ceil{t}~\ceil{t'}
  \end{eqnarray*}
  \caption{Lifting values}
  \label{fig:valuelift}
\end{figure}

First of all, analogous to the lifting of types to the logic level, we define
a lifting of values to the logic level. Similarly, this translation traverses
the value until it encounters an impure function. From the logic point of
view, an impure function is represented by its specification, so the
translation keeps only the pair of pre- and postcondition, both abstracted
over the function argument. The function body is thrown away.

\begin{figure}[tbp]
  \begin{eqnarray*}
    c(x~[\alist{κ}]) &=& \Trueml \\
    c(λx:τ.t) &=& ∀x:\ceil{τ}.c(t) \\
    c(t~t') &=& c(t) /\ c(t') \\
    c(\recml~f~(x:τ).\{p\}(t : τ',ε)\{q\}) &=&
    ∀x:\ceil{τ}.∀f:\ceil{τ->^ετ'}.  f = \ceil{v} => \\& &
    ∀s:\efft{ε}.p~s => \wpre_s(t,q~s) \\
    \wpre_s(v, q) &=& c(v) /\ q~s~\ceil{v}\\
    \wpre_s( t_{τ'->^ε τ} t' ,q) &=& \correct{t} /\ \correct{t'} /\
    fst~\ceil{t}~\ceil{t'}~s /\ \\ 
    & & ∀s':\efft{ε}∀ x:\ceil{τ}.~  snd~\ceil{t}~\ceil{t'}~s~s'~x => q~s'~x\\
    \wpre_s(\letml~x~[\alist{χ}] = v~\inml~t, q) &=&
      ∀\alist{χ}.c(v) /\ \letml~x~[\alist{χ}]=\ceil{v}~\inml~ \wpre(t,q)\\
    \wpre_s(\letml~x = ( t1 : τ,ε) ~\inml~ t2 , q) &=&
    \wpre_s( t1 ,λs':\efft{ ε }.λx:\ceil{τ}.\wpre_{s'}( t2 , q ))\\
    \wpre_s(\ifml~ t1 ~\thenml~ t2 ~\elseml~ t3 , q) &=&
      \ifml~ \ceil{ t1 }~\thenml~ \wpre_s( t2 , q)~\elseml~\wpre_s( t3 , q) \\
      \wpre_s( (t : ε <: ε'), q) &=& \wpre_{s|_ε}(t, λs':\efft{ε}.q~(s\oplus s'))
  \end{eqnarray*}
  \caption{The weakest precondition calculus}
  \label{fig:wp}
\end{figure}

The weakest precondition calculus itself comes in two parts, one for values
and one for effectful terms. Both are reasonably straightforward. The {\em
correctness} of values, written $c(v)$, ensures that any effectful function
appearing in a value adheres to its specification. To this end, the value is
traversed, quantifying over bound variables and using conjunction for pure
applications. When an effectful function is encountered, one has to prove that
the precondition of the function implies the postcondition, given the function
body.

The part concerning effectful functions, denoted $\wpre_s(t,q)$ takes as
argument a term $t$ of type $τ$ and effect $ε$, a postcondition $q$ of type
$\efft{ε} -> τ -> \propml$ and a current state $s$ of type $\efft{ε}$. If the
term is in fact a value, the value has to be correct and the postcondition
must be true for the lifted value and the current state. In the case of an
application, both values must be correct, the precondition of the function
must hold in the current state, and for any state and return value of the
function, the function's postcondition must imply the condition $q$ to be
proved.

As for the typing relation, we have two rules for the \letml-binding. If the
bound term is a value, we can translate the \letml-binding in the program by a
\letml-binding in the logic. In the other case, the \wpre\ of $ t2 $ becomes
the argument of the \wpre\ of $ t1 $ (we denote $ε$ the effect of $ t1 $ and $
t2 $). The rule for \ifml-expressions is straightforward. The rule for
subeffecting adapts the domain of the current store and the expected domain of
the postcondition so that the types work out.

We do not give a correctness proof for this calculus; this proof would be very
similar to and actually simpler than the proof given in the next section.

\section{The Alias Restriction}
\label{sec:alias}

The presented programming language, together with the weakest precondition
calculus, form a basis for program verification and can already be used to
prove programs. On the other hand, in its presented form, and because of the
presence of aliasing, one would constantly need equalities and disequalities
between reference variables to be able to reason about a program.

\begin{figure}[tpb]
  \begin{mathpar}
    {\inferrule*[Left=Var]
      {Γ(x) = ∀\alist{χ}.τ \\ σ = [\alist{χ}|->\alist{κ}] \\ σ \sim τ }
      {Γ |- x : τσ }
    } \\
    { \inferrule*[Left=LetRef, Right={ρ \textup{fresh}, $ρ\notin τ$}]
      {Γ |-_v t1 : τ' \\ Γ, x : τ'~\refml_ρ |- t2 : τ, ε }
      {Γ |- \letml~x = ref~ t1 ~\inml~ t2 : τ, ε \setminus ρ}
    }
  \end{mathpar}
  \caption{A modified and a new rule for aliasing}
  \label{fig:aliasing}
\end{figure}

A way out is to restrict the attention to alias free programs. Fortunately, it
is easy to modify the typing rules to render all programs with aliasing
ill-typed (see Fig~\ref{fig:aliasing}). In the following, we will use regions
in such a way that there is exactly one memory location in each region.
Starting from this assumption, a program can actually exhibit {\em two} forms
of aliasing. In the first one, it may use several names (region variables) for
the same memory location. This can happen when a function is generalized over,
say, two region variables, assuming they are different, and is then
instantiated with the same region variable twice. The classical example is the
following program
\begin{who}
  let f [ρ ρ'] (x : int ref(ρ)) (y : int ref(ρ')) =
  { }
  x := !x + 1;
  !y
  { r : r = !!y old }
\end{who}
which is only correct if $ ρ $ and $ ρ' $ are indeed different. To enforce
this, we add a side condition to the instantiation: the substitution $σ$ must
be {\em compatible} with the type $τ$, written $σ\sim τ$. To understand what
compatibility means, consider an {\em atomic} region substitution (whose
domain contains only one variable) $[ρ|->ρ']$. It is compatible with any type
which does not contain $ρ'$. Similarly, the effect substitution $[e|-> ρ1  ρ2
e'$] is compatible with any type which does not contain $ ρ1 , ρ2 $ nor $e'$.
Expressed more generally, an atomic substitution is compatible with a type if
the type does not contain any region or effect variables which are in the
range of the substitution. Finally, a {\em composite} substitution (whose
domain contains more than one variable) is compatible with a type if each
atomic substitution is compatible with the result of the application of the
previous substitution. In this way, it becomes impossible to have different
names for a single reference cell.

The second form of aliasing is when we use the same name for {\em different}
memory locations. This can happen when we allow the user to call
$\mathit{ref}$ several times with the same region instantiation. A simple
possibility is to  suppress the $\mathit{ref}$ function entirely and to
replace it by a \letml\refml\ construct which combines name creation and
memory allocation. In this way, there can of course be only one reference per
name. On the other hand, because of the side condition $ρ\notin τ$, the
reference cannot escape the scope of its \letml\refml\ construct, and so, a
function cannot return a reference which has been created in its body. This
restriction could be leveraged a bit if one separates name creation and
allocation, while still maintaining that one can only allocate a single
reference in each region. This is how it is done in the correctness proof of
this restriction. The presented \letml\refml\ construct is of course a special
case.

In alias-free programs, the operations $\oplus$ and $|_ε$ on stores can be
executed {\em statically}, i.e. just by looking at the domains of the stores.
Also, we statically know which value (or which named component) the accessor
{\tt !!} will get from a store. Therefore, reasoning about stores and
equalities between regions can be completely eliminated.

With the aliasing restriction, complex programs using shared mutable data
structures are impossible to prove. Programs with references in lists,
references in references, etc., can be written, but only with the severe
limitation that a list can only contain a single reference (potentially
several times) and that a reference can only contain always the same
(statically known) reference. However, references still are first-class
values, i.e. they can be passed to functions, returned by functions and stored
in data structures, with the above limitations.

\paragraph{Correctness.}

We have proved the correctness of our system. All the properties are proved in
subject reduction style, maintaining an invariant during the execution of the
program, modeled with small-step semantics. We only consider well-typed terms.

For the \wpre-calculus, we want to obtain that if we can prove $\wpre_s(t,q)$
where $s$ is the current state of the store, $t$ a program, and $q$ the
postcondition, and if $t$ reduces to the (syntactic) value $v$ and the store
$s'$, then $q~s'~v$ is valid. The subject reduction way of proving this
proceeds by proving that the validity of the formula $\wpre_s(t,q)$ is in fact
maintained during the execution of the program. However, in the presence of
effect typing, the effect of $t$ may {\em decrease} during program execution.
Thus, the main lemma for the \wpre-calculus is 
\begin{lemma} 
  $\wpre_s(t,q)$ and $s,t ->> s',t'$ implies $\wpre_{s'}( (t : ε'<:ε), q)$.
\end{lemma}
Here, $->>$ means ``reduces in zero or more steps to''. The main theorem
is then an easy consequence.

For the alias restriction, or more precisely the restriction on reference
creation, we formally prove that each region contains exactly one memory
location, i.e. is actually a {\em singleton region}. Here we have to
instrument the semantics and mark each reference cell in the store with the
region it belongs to; therefore the exact statement is the following:
\begin{theorem}
  If $s,t ->>s',t$ and if $s$ contains only singleton regions (each reference
  cell maps to a different region), then this is also the case for $s'$.
\end{theorem}

Both theorems are proved in the technical appendix~\cite{appendixesop2010}
accompanying this paper. The aliasing restriction is proved there in a more
general setting, while the correctness of the \wpre-calculus is proved in a
slightly different version.

\begin{figure}[tpb]
\begin{who}
type ('a,'b) map
logic mem : forall ('a, 'b) . 'a -> ('a,'b) map -> bool
logic get : forall ('a, 'b) . 'a -> ('a,'b) map -> 'b
logic set : forall ('a, 'b) . 'a -> 'b -> ('a,'b) map -> ('a,'b) map

let stores f0 m =
  forall x : int, mem x m = true -> get x m = f0 x

parameter table : (int,int) map ref(ρ)

let memo (f0 : int -> int) (x : int) = 
  {stores f0 !!table}
  if mem x !table then get x !table
  else
    let z = f0 x in
    table := set x z !table;
    z
  {r : r = f0 x /\ stores f0 !!table}
\end{who}
  \caption{The code along with the specification of the {\tt memo} function}
  \label{fig:memo}
\end{figure}

\section{Examples}
\label{sec:examples}

In this section, we will show two simple examples which use higher-order
features.

\subsection{A Simple Memoization Function}

Our first example is a simple memoization function {\tt memo} for (pure)
functions.  It takes as an argument a function {\tt f} (from {\tt int} to {\tt
int} here) and an integer and returns the same result as {\tt f} would;
however, before executing the argument, it looks up the integer in a table to
see if the function hasn't been called yet on this integer. If so, the table's
value is returned, otherwise we call the function, store the result in the
table, and return it. Fig~\ref{fig:memo} shows the code along with the
specification. Since we are not interested in the implementation of the table,
we simply model it by a reference to a functional map with mem/get/set functions.

The specification of {\tt memo} is very simple. The only precondition is that
the table should only contain bindings $y |-> {\mathtt f}~y$, which is
expressed by the {\tt stores} predicate. The postcondition states that the
result is equal to {\tt f0~x} and that the table still verifies the {\tt
stores} predicate.

\subsection{Memoization of Recursive Functions}

\begin{figure}[tpb]
\begin{who}
  let realizes (f0 : int -> int) (f : [[ int ->{table} int ]]) =
    forall (x : int),
    { stores f0 !!table } f x { r : r = f0 x /\ stores f0 !!table }

  let ymemo (f0 : int -> int) 
            (ff : (int ->{table} int) ->{} int ->{table} int) = 
    { forall (k : [[int ->{table} int ]]),
        { realizes f0 k} ff k { r : realizes f0 r} }
    let rec f (x : int) =
      { stores f0 !!table }
      if mem x !table then get x !table
      else
        let z = ff f x in
        table := set x z !table; z
      {r : r = f0 x /\ stores f0 !!table}
    in f
  { rf : realizes f0 rf }
\end{who}
\caption{The {\tt ymemo} function and its specification}
  \label{fig:ymemo}
\end{figure}

Our second, more complicated example is a version of {\tt memo} for recursive
functions, dubbed {\tt ymemo}. We want to be able to define a recursive
function whose recursive calls are automatically memoized. The basic idea of
{\tt ymemo} is analogous to the fixpoint combinator in λ-calculus, hence the
name. As an example, here is how one would implement a memoized Fibonacci
function using {\tt ymemo}:
\begin{ocaml}
  let fib = 
    ymemo (fun g x -> if x <= 1 then x else g (x-1) + g (x-2))
\end{ocaml}
The anonymous function uses its first argument {\tt g} to do recursive calls.

{\tt ymemo} achieves this by defining a recursive function {\tt f} inside its
body (see Fig~\ref{fig:ymemo}). This function, when called with an integer
argument {\tt x} first checks if the table contains {\tt x}, and if not, calls
{\tt ff} {\em with {\tt f} itself} and x as argument. As before, it stores the
obtained result in the table and returns it. Now, the return value of {\tt
ymemo} is precisely this recursive function {\tt f}. The only difference with
the equivalent \ocaml\ code is the usage of a logical argument {\tt f0} which
stands for the (mathematical) function to be defined.

Let us move on to the specification of {\tt ymemo}. We first define the {\tt
realizes} predicate between a pure function {\tt f0} and an impure one
modifying the table. This predicate states that {\tt f} {\em realizes}, i.e.
returns the same result as, {\tt f0} and additionally, if the table stored
valid key/value pairs, this is also the case after executing {\tt f}.
Actually, this is precisely the specification of {\tt memo}, so we could
easily prove {\tt realizes f0~(memo f0)} for any pure {\tt f0}. The definition
of the predicate uses the syntactic sugar for Hoare triples we introduced in
section~\ref{sec:genpic}. 

The recursive function {\tt f} in the body of {\tt ymemo} has the same spec as
{\tt memo}, and indeed here it is trivial to prove the postcondition of {\tt
ymemo}, which states that its return value realizes {\tt f0}. Now, in order to
prove that the specification of {\tt f} is indeed correct, the only
information we need is that {\tt ff k} realizes {\tt f0} if {\tt k} is a
function which realizes {\tt f0}.

\subsection{All Examples - An Overview}

A prototype implementation of the presented system, called ``\who'', is freely
available~\cite{KanigFilliatre09wml}\footnote{It is available at
\url{http://www.lri.fr/~kanig/who.html}.}. It implements the described
language with type, region and effect polymorphism, higher-order logic and
store types in the logic. It is currently restricted to the subset
characterized by the alias restriction. It takes as an input a fully annotated
program and outputs proof obligations which have to be proved to guarantee the
correctness of the program with respect to its specification. Currently, only
output to the interactive proof assistant Coq has been implemented. Our logic
is a subset of Coq's, so the translation is particularly simple.  The
implementation consists of less than 3000 lines of \ocaml\ code, including a
limited form of type and effect inference.

\begin{figure}[tbp]
\begin{center}
    \begin{tabular}{ | r | r | r | r |}
    \hline
    Program & Ocaml Code & Code + Spec in \who & Proofs \\ \hline
    Memo/ymemo & 15 loc & 26 loc & 90 lop  \\ \hline
    {\tt Array} & 69 loc & 170 loc  & 320 lop  \\ \hline
    Koda-Ruskey~\cite{KanigFilliatre09wml} & 10 loc & 30 loc & $\sim$700 lop  \\ \hline
    \end{tabular}
\end{center}
  \caption{A summary of all programs proved so far using the \who\ tool}
  \label{fig:whoproofs}
\end{figure}

Fig.~\ref{fig:whoproofs} shows an overview over the programs we have been able
to prove using the \who\ tool. The first line summarizes the numbers of the
memo/ymemo example of the previous subsections. 

The next two lines concern the two modules {\tt Array} and {\tt Hashtbl} of
the \ocaml\ standard library. For the {\tt Array} module, all functions
present in the \ocaml\ module have been proved, with the exception of the {\tt
Array.map} family, whose specification is very similar to {\tt iter}, and the
{\tt Array.sort} function, which is quite complicated to prove, but not very
interesting concerning higher-order functions or side effects. In this proof
development, arrays are modeled as references to functional arrays with the
usual axioms on access and update.

%The proof development concerning the {\tt Hashtbl} module is ongoing work. The
%difficult part of this development is the model of the \ocaml\ hash table; In
%\ocaml, when one overwrites a previously existing entry in the table, the old
%value still exists, and is only {\em shadowed} by the new key/value pair. If
%one calls {\tt Hashtbl.remove} using this key, the old value reappears. Also,
%if several bindings for a key exist, the key may be encountered several times,
%during a call of {\tt Hashtbl.iter}, for example. A possible way of modeling
%this is an association list instead of a simple mapping, where keys in front
%of the list can shadow the same key further down in the list. 
% Currently, only
%two functions of this module have been proved correct.

Finally, the last line shows the numbers for the proof development of the
Koda-Ruskey algorithm~\cite{KodaRuskey93}. A small, efficient
implementation~\cite{FilliatrePottier02} is shown to be correct. More details
about this development can be found in~\cite{KanigFilliatre09wml}.

\section{Related Work}
\label{sec:related}

Honda, Yoshida and
Berger~\cite{Honda05anobservationally,BergerHondaYoshida05aliasing} present a
logic for imperative higher-order programs. They present their calculus using
rules in Hoare logic style, while we presented a \wpre-calculus. Their system
does not provide the same modularity than ours, as one can not reason
modularly about side effects of functional arguments. It also has the drawback
of being difficult to implement, due to the presence of a large number of
non-structural rules. Our system, starting from a correctly annotated program,
is syntax-directed and thus entirely automated. Their logic of proof
obligations is non-standard, while we obtain obligations in standard
higher-order logic. They deal with aliasing using equalities and disequalities
between pointers.  They can reason about the classic {\em Landin's knot},
which consists in obtaining recursion by storing a function in a reference
which calls this same reference. Our typing restriction prohibits this
particular kind of programs.  Honda, Yoshida and Berger prove a completeness
result for their system. We have proved no such result.

The Coq proof assistant~\cite{CoqManualV81}, in particular in combination with
the Program extension~\cite{sozeau07icfp}, can be used to implement and prove
arbitrary properties of purely functional programs, using dependent types. The
appeal of that approach is that the only limit of expressiveness is the one of
Coq. One can also do meta-reasoning about programs inside Coq and, at the end
of the development, extract a certified implementation in \ocaml\ or Haskell.
The confidence in programs proved in this way is very high, because in the end
one obtains a Coq proof term that corresponds to the correctness of the
program. This term has been checked by Coq's kernel. However, without
extensions such as Ynot (see next paragraph), it is impossible to implement
(and reason about) effectful computations. Another drawback of this approach
is lack of automation.

The Ynot System~\cite{Nanevski08Awkward,chlipalaicfp09} is an extension to the
Coq proof assistant, capable of reasoning about imperative higher-order
programs, including effectful functions as arguments, using a monad in which
effectful computations can take place. It uses higher-order separation logic
as the device to speak about modifications of the state. Ynot uses an
(arguably less intuitive) monadic representation of effectful computations.
Ynot is more expressive than our system because it uses Coq as the underlying
system, and features modular reasoning (abstracting over the specification and
effect of a function as argument) while being able to reason about aliasing
situations thanks to separation logic. Ynot also has limited support for
proof automation, thanks to tactics dealing with separation logic
formulas~\cite{chlipalaicfp09}. We claim that we can obtain much better
automation for alias-free programs, and even higher-order functions. We
explain in the next section how we plan to achieve this.

The Pangolin system, the implementation of the theoretical system
of~\cite{regis-gianas-pottier-08}, can also be used to reason about
purely functional programs. It is also one of the starting points of
our work, and our purely functional fragment is basically the Pangolin
system, although we removed algebraic data types for clarity of
presentation. For this reason, we share the potential advantages (good
automation, relatively simple system) and drawbacks (no
meta-reasoning or observational equality, no machine-checked proof
term). Pangolin cannot deal with side effects.

Another line of work consists in giving a translation from an imperative to a
functional language, with the aim of applying one of the techniques for purely
functional programs to the translation. O'Hearn and
Reynolds~\cite{ohearn-reynolds-00} present a translation from Algol with first
class references and dynamic allocation to a linear $\lambda$-calculus. They
impose an alias-avoiding restriction which is similar to ours. Charguéraud and
Pottier~\cite{chargueraud-pottier-08} give a translation from Core ML with
first-class references, dynamic allocation and aliasing to a functional
language, using a very expressive type system with linear capabilities and
group regions, while we only accept singleton regions. The second system can
deal with aliasing programs. However, none of these works describe a mechanism
to obtain proof obligations, for example a \wpre-calculus.  We present a
complete cycle from imperative programs to proof obligations. We also claim
that extending our language and our \wpre-calculus is simple, as long as
aliasing is tracked precisely.  We leave these extensions for future work.

Systems for verification of first-order programs like
Why~\cite{Filliatre00a,filliatre07cav} or Spec\#~\cite{BarnettLS04} are the
other starting points of this work. These systems generally deal very well
with usual features of first-order programs, for example arrays, use SMT
solvers to discharge proof obligations and strive for the best possible
automation. We improve on this work by adding higher-order features,
while trying to maintain the degree of automation and ease of use.

\section{Conclusion}
\label{sec:conclusion}

We have presented a Hoare logic for higher-order programs with side
effects. Its main feature is the ability to reason about higher-order
functions in a {\em modular} way. A higher-order function can be
specified by abstracting over the specification and the effect of its
arguments. This specification can be reused when the higher-order
function is instantiated with a particular effect and applied to a
particular argument. This modularity is achieved by combining two
previously known mechanisms. Effect polymorphism enables us to
abstract over the modifications of a function. Reflecting values and
types in a higher-order logic enables us to abstract over the
specification of a function. We have proposed a way to
reason about the evolution of a store, with potentially unknown
components represented by effect variables. We have limited ourselves to
programs without aliasing.

In its simplicity and its drawbacks, our system is similar to the one
of ~\cite{regis-gianas-pottier-08}. The system's definition holds
entirely in Figure~\ref{fig:wp}, and it can be easily
implemented because it is syntax-directed. On the other hand, we only
consider partial correctness. Also, we can not reason about
observational equivalence of programs. In our system, two functions
are equivalent if their specifications are.


\paragraph{Future Work.}

We are currently investigating the possibility to obtain first-order proof
obligations, to be able to use SMT solvers~\cite{RanTin-SMTLIB}, which would
avoid manual proofs in many cases. An obvious solution would be to reuse
existing encodings of higher-order logic in first-order logic, such as the one
by Pottier and Gauthier~\cite{pottier-gauthier-hosc}. This technique has been
successfully employed by the Pangolin system~\cite{regis-gianas-pottier-08}.
Another option comes from our observation that in many cases, the proof
obligations for higher-order functions can actually be expressed in
first-order logic, when one replaces top-level {\em quantifications} of
functional arguments and invariants by top-level {\em declarations}. Doing
this translation manually, we have obtained first-order proof
obligations for the {\tt Array.iter} function which are all discharged by
SMT solvers. It is future work to find out if such a transformation can always
be carried out.

Currently, our effect system is quite limited. One possible extension is the
addition of subtyping: an effectful function $f$ of type $τ ->^ετ'$ can be seen
as a function of type $τ ->^{ε'}τ'$, if $ε\subseteq ε'$. In particular, such a
subtyping operation should come at no cost for program verification; it should
still come for free that $f$ does not modify the variables in $ε'\setminus ε$. It
would also be interesting to see if an ``impure'' function of type $τ
->^\emptyset τ'$ can be seen as pure function $τ -> τ'$ (and be used in the
logic).

We see two possibilities to deal with aliasing beyond plain usage of pointer
disequalities. The first one is to modify the effect system itself to allow
more precise tracking of aliasing. In the setting with the aliasing
restriction, our regions actually become {\em singleton regions}, regions
containing exactly one memory location. Other, more powerful effect systems
allow for {\em group regions}, with the possibility to temporarily {\em focus
} on a particular memory cell in that region. We see no fundamental difficulty
in adopting such an effect system, but haven't worked out the details. A
second possibility is to use techniques of static analysis to infer disjoint
regions, each possibly containing several pointers~\cite{hubert07hav}.
This approach has proved useful in combination with other separation
techniques, for example the Burstall-Bornat memory model~\cite{bornat00mpc}
which separates automatically objects of different structural types.

Finally, from a programmer's point of view, it would be nice to have more
features in the programming language, such as recursive types and algebraic
data types. As long as defined types verify a {\em positivity
condition}~\cite{paulintlca93}, these extensions would be completely
orthogonal to the weakest precondition calculus and the type and effect system
used.

\bibliographystyle{plain}
\bibliography{biblio}{}

\end{document}
