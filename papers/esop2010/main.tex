\documentclass[a4paper]{llncs}

\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc} 
\input{utf8symbols}
\usepackage{stmaryrd}
\usepackage{amsmath,amssymb}
\usepackage{mathpartir}

\newcommand{\wpre}{{\bf wp}}
\newcommand{\sep}{ ~|~ }
\newcommand{\letml}{{\bf let}}
\newcommand{\inml}{{\bf in}}
\newcommand{\ifml}{{\bf if}}
\newcommand{\thenml}{{\bf then}}
\newcommand{\elseml}{{\bf else}}
\newcommand{\refml}{{\bf ref}}
\newcommand{\preml}{{\bf pre}}
\newcommand{\postml}{{\bf post}}
\newcommand{\propml}{{\bf prop}}
\newcommand{\boolml}{{\bf bool}}
\newcommand{\recml}{{\bf rec}}
\newcommand{\efft}[1]{\langle #1 \rangle}
\newcommand{\lift}[1]{\langle #1 \rangle}
\newcommand{\alist}[1]{\overline{#1} }
\newcommand{\Trueml}{{\bf True}}
\newcommand{\Falseml}{{\bf False}}
\newcommand{\trueml}{{\bf true}}
\newcommand{\falseml}{{\bf false}}
\newcommand{\unitml}{{\bf unit}}
\newcommand{\intml}{{\bf int}}
\newcommand{\coq}{Coq}
\newcommand{\who}{Who}
\newcommand{\ocaml}{Ocaml}
\newcommand{\correct}[1]{c(#1)}

\newcommand{\ceil}[1]{\lceil #1 \rceil}

\begin{document}
\title{\mbox{Specifying Imperative Higher-Order Programs}}

\author{Johannes Kanig\inst{1,}\inst{2} \and Jean-Christophe
Fill\^atre\inst{2,}\inst{1}}
\institute{
INRIA Saclay - Île-de-France, ProVal, Orsay, F-91894 \and
 LRI, Univ Paris-Sud, CNRS, Orsay, F-91405
}
%TODO explain somewhere the correspondence store type <-> map, effect variable
%<-> domain, region <-> key

%external TODO : proofs for Hashtbl
%external TODO : proofs of system
% 


\maketitle

\begin{abstract} We present a specification language suited for reasoning
  about effectful higher-order programs. The language's main features are
  higher-order logic and effect polymorphism, both of which enable a truly
  modular treatment of higher-order functions.  We also define a weakest
  precondition calculus for this language which generates proof obligations in
  higher-order logic.  To our knowledge, this is the first time that effect
  polymorphism is put to use in program verification.  A simple mechanism
  based on regions permits to restrict the analysis to alias-free programs,
  thus obtaining much simpler proof obligations. Several examples demonstrate
  the practicality of the system. A prototype implementation exists and has
  been used to prove the correctness of these and more examples.
\end{abstract}

\section{Introduction}

The purpose of the present paper is to introduce a simple weakest precondition
calculus for imperative higher-order programs. Higher-order functions are a
key feature, not only of purely functional languages like Haskell and the pure
fragment of ML, but also in combination with imperative features, like
references. However, reasoning about higher-order functions is difficult, and
especially so in the presence of effectful computations. Any function's
specification depends on its arguments. In a higher-order setting, arguments
may be functions that have their own specification. In consequence, to be
modular, the specification of a higher-order function can only be formulated
{\em depending on the specifications of its functional arguments}. In a
setting where side effects are possible, one also needs to formulate the
specification {\em depending on the side effects of the functional arguments}.
We claim that to be truly modular, the specification of a higher-order
function must be generic with respect to the specification {\em and} the
effect of its arguments.

In this paper, we first present a specification language combining features of
a programming language and logic. It uses an effect system and a region
calculus to track references. The effect system contains {\em effect
polymorphism}, thus dealing well with higher-order functions. The
specifications can be expressed in higher-order logic, to be able to state
properties of functional arguments. We show how two very simple modifications
to the system let us obtain alias-free programs, with the aim of obtaining
much simpler proof obligations.

We then proceed to define a weakest precondition calculus based on this
language, transforming an annotated program into a logic formula, which has to
be valid for the program to be correct. This calculus is very straight-forward
and syntax-directed.

Finally, we show the usefulness of the system by giving several examples of
annotated programs. All the programs have been proved correct using our
prototype implementation called \who.

\paragraph{Outline.}
Section~\ref{sec:genpic} gives an introduction to the described language as
well as an example. Sections~\ref{sec:syntax} and \ref{sec:wp} give the
details about the programming language and the weakest precondition calculus.
In section~\ref{sec:examples} we speak about the implementation and show some
examples. Finally, section~\ref{sec:related} discusses related work.

\section{The general picture and a motivating example}
\label{sec:genpic}

Consider the following first-order program\footnote{Throughout the document,
we use \ocaml~Syntax for programs.} that accumulates the sum of the contents
of some array $a$ in some global reference $c$, using an integer $i$ as
a counter:
\begin{ocaml}
  let sum () = 
    for i = 0 to Array.length a - 1 do
      c := !c + a.(i)
\end{ocaml}
This simple  can be annotated in a standard Hoare logic (for
example the system of \cite{Filliatre00a}) as follows:
\begin{who}
  let sum () =
  {!!c = 0}
    for i = 0 to length !a - 1 do
    { !!c = {{{ $\sum_0^{i-1}$ get !!a i }}} }

      c := !c + get !a i

      {!!c = {{{ $\sum_0^{ {\tt length~!a} - 1}$ get !!a i }}} }
\end{who}

Here, assuming we model the array in the programming language by a reference
to a functional array, with functions {\tt length} and {\tt get} with the
obvious meaning. Also, in the logic we use a derefence operator {\tt !!}
instead of the usual {\tt !}. In this case, our program is a {\tt for} loop,
so it needs an {\em invariant annotation}, indicating which relation is
preserved by the loop body. The equalities that need to be proved are very
simple. Also, it is obvious that the program modifies exactly the references
$r$ and $c$, and this fact can be formally confirmed by an effect analysis.

A common reaction of a functional programmer to a program like
$\mathit{sum}$ is to factor out the structure of the recursion, which
may be useful in other situations, in a higher-order function. The
more specific program is then recovered by function application. In this case,
the more general function is of course simply {\tt Array.iter}:
\begin{ocaml}
  let iter f a = 
    for i = 0 to Array.length a - 1 do
      f a.(i)

  let sum () =
    iter (fun n -> c := !c + n) a
\end{ocaml}
It is obvious that the two programs do exactly the same
thing. However, how do we prove the correctness of the second one? One
possibility is to fix the future usage of ${\tt iter}$ by
specifying what exactly is the specification of its argument {\tt f}:
\begin{who}
  let iter 
    (f : (x : int) ->{c} 
        { !!c = {{{ $\sum_0^{i-1}$ get !!a i }}} } 
        unit 
        { !!c = {{{ $\sum_0^{length !a - 1}$ get !!a i }}} } ) 
    (a : int array) =
    ...
\end{who}
Here we impose precisely the specification of {\tt f}, and thus we can
derive that {\tt sum} computes the right value,
using the same method as for the first-order case before. Note
that, to be able to apply an effect analysis, we have to specify that
$f$ modifies $c$, by giving it the type $\intml->^c\unitml$.

Of course, this approach is not very satisfactory. While the program achieves
the separation of the iterating function ({\tt iter}) and the combining
function (the anonymous function), the specification does not do this at all;
as specified,  {\tt iter} can basically only be applied to the same function,
and can only compute a certain sum. If one wants to use {\tt iter} in another
context, one has to copy the code, give a new specification adapted to the
application and prove this new specification again. In addition, in order to
be able to write the specification at all, we had to specialize the type of
{\tt iter} to integer arrays, while its \ocaml\ counterpart is of course
polymorphic. Said otherwise, this approach is not modular at all.

There are two obstacles related to higher-order functions to be
overcome in order to achieve modularity on the level of specification
in the presence of side effects. We must be able to refer to the
effect of a function parameter, and we must be able to refer to the
specification (pre- and postconditions) of a function parameter, in a
generic way.

The first obstacle has a well known solution using a type and effect
system~\cite{talpin94taed} with effect variables: effect polymorphism
does exactly what we want here. Using effect polymorphism, we can say
that {\tt f} (the argument of {\tt iter}) has type
$\intml->^e\unitml$, where $e$ is an {\em effect variable}, which
amounts to say that {\tt f} has a {\em certain} effect $e$. The total
effect of {\tt iter} is then $e$, meaning {\tt iter}
modifies whatever {\tt f} modifies.

A solution to the second problem concerning specifications consists in
representing a program function in the logic as the pair of its pre- and
postcondition. This is an old idea, and it has recently been applied to purely
functional programs by Régis-Gianas and
Pottier~\cite{regis-gianas-pottier-08}. 

The main idea of this paper is to combine the effect-based Hoare logic of
\cite{Filliatre00a} with the more powerful effect system of
\cite{talpin94taed} and the more powerful annotations of
\cite{regis-gianas-pottier-08} to obtain a modular system for proving
higher-order programs with side effects. As in~\cite{regis-gianas-pottier-08},
we use operators \preml\ and \postml\ to access the pre- and postcondition of
a function. However, in our system, a precondition takes as an argument not
only the function argument, but also the initial state, before the execution
of the function. The postcondition takes the argument and return value, but
also the states before {\em and} after the function call. Th so-called
two-state postconditions are a way to get rid of most uses of auxiliary
variables. However, the pre- and postcondition of {\tt f} may not refer to
arbitrary parts of the state, but only the part {\tt f} reads or modifies.
With these conventions, one can now write a generic specification for the {\tt
iter} function, see ~Fig.~\ref{fig:iter}.

\begin{figure}[tbp]
\begin{who}[lines]
let iter [e] (a : 'a farray ref) (inv : <e> -> int ->  prop)
  (f : 'a ->{e} unit) =
  { inv cur 0 /\ forall (i:int). 0 <= i /\ i < length !!a ->
    forall (m : <e>).  inv m i -> pre f (get i !!a) m /\
    forall (n : <e>).  post f (get i !!a) m n () -> inv n (i+1)
  }
  for index = 0 to length !a - 1 do
    { inv cur index }
    f (get index !a)
  done
  {inv cur (length !!a) }
\end{who}
\caption{The specification of the {\tt iter} function}
  \label{fig:iter}
\end{figure}

There are several new elements here. First of all, we use effect polymorphism
to introduce an effect variable {\tt e} standing for the effect of {\tt f}.
Also, {\tt iter} has an additional argument {\tt inv}, the {\em invariant},
taking as argument the portion of the state modified by f and the integer
indicating the current iteration count. This is a logical argument, which
means that it cannot be used in the program. Also, {\tt inv} is a {\em pure}
function, which is indicated by the arrows without effect annotation. The
postcondition is now quite clear: we simply want to establish the invariant
for the final state (the keyword {\tt cur} for the current state of the
postcondition) and the length of the array, indicating that we have iterated
over the entire array. It is also easy to see that the invariant {\tt inv} is
precisely the invariant of the {\tt for} loop. However, the precondition is a
bit complicated: it states that we need the invariant to hold initially (for
the current state of the precondition and index $0$) and we need the invariant
to be preserved by the function call. This is expressed by quantifying over
two states, saying that the invariant has to imply the precondition of {\tt f}
and the postcondition has to imply the invariant for the next array index.
With a bit of syntactic sugar, this could be expressed much more concisely
using a Hoare triple:
\begin{who}
  { inv cur i} f (get i !!a) {inv cur (i+1)}
\end{who}

The {\tt iter} function and its specification raise a question: is the
specification correct if {\tt f} modifies the array {\tt a}, which is required
for the good behavior of {\tt iter}? The answer is clearly no; if {\tt f}
messes up the contents of {\tt a}, {\tt iter} may not actually maintain the
invariant.  This is an instance of the {\em aliasing problem} (see, for
example, \cite{reynolds78popl}) and it is well known that standard Hoare logic
does not support aliasing. In this paper we make the same choice and disallow
aliasing in the programming language we introduce. Under this assumption, the
specification we gave is correct. 

Entirely excluding aliasing renders our system inappropriate for
reasoning about shared mutable structures. However, it is interesting
to see that our alias exclusion restriction, in many cases,
corresponds to a natural condition imposed on higher-order functions
that operate on some data structure. It is a common
requirement~\cite{Nanevski08Awkward,Krishnaswami06} for iteration
functions, for example, that the functional argument must not modify
the structure to be iterated on. In our system, this fact follows from
the restriction on effect instantiations, as will be seen in
section~\ref{sec:syntax}. In the example, the array {\tt a}
of the iterating function {\tt iter} must not be modified by its
argument.

\section{A programming language for specification}
\label{sec:syntax}

%TODO  notion of substitution - correspondence χ κ
\subsection{Syntax}
\begin{figure}[tpb]
\begin{eqnarray*}
  x & & \text{Program Variables}\\
  α & & \text{Type Variables}\\
  e & & \text{Effect Variables}\\
  ρ & & \text{Region Variables}\\
  χ &::=& α \sep e \sep ρ \\
  t,p,q &::=& x~[\alist{κ}] \sep λx:τ.t \sep \recml~f~(x:τ).\{p\}t\{q\} \sep t~t\sep \\ 
  & & \letml~ x~[\alist{χ}] = t~\inml~t \sep \ifml~t~\thenml~t~\elseml~t\\
  τ &::=& α \sep τ -> τ \sep τ ->^ε τ \sep τ \times τ \sep τ~\refml_ρ \sep
  \efft{ε} \\
  ε &::=& (ρ \sep e)*\\
  κ &::=& τ \sep ε \sep ρ
\end{eqnarray*}
  \caption{Syntax}
  \label{fig:syntax}
\end{figure}

Our programming language (Fig.~\ref{fig:syntax}) is basically the λ-calculus
with \letml-bindings, where there are two different λ-bindings. The first one
($λx:τ.t$) describes a pure, non-recursive binding, and the second one
($\recml~f~(x:τ).\{p\}t\{q\}$) describes recursive, potentially impure
functions; $p$ and $q$ in this term are {\em specifications}, which are
of the same syntax as programs, but are differently typed (typing will be
discussed in the next subsection). The term $p$ corresponds to the {\em
precondition}, $q$ to the {\em postcondition} of the function. The analogy to
Hoare triples is of course obvious. As usual, there are also variables,
\letml-bindings and an \ifml-\thenml-\elseml-construct. Variables on the type
level can be generalized at \letml-boundaries and have to be instantiated when
the corresponding program variable is used.

Before we describe the syntax of types, let us first explain {\em regions} and
{\em effects}. Regions are simply type level names for memory locations. An
effect is a set of memory locations and {\em effect variables}. An effect
variable stands for an unknown effect. Effects are used to specify
of memory locations an expression can write. They are also used in function
types to describe the memory locations which may be written if the function is
called.

Type, region and effect variables must be explicitly generalized and
instantiated. For this purpose, we have defined the metavariable $χ$,
representing variables, and $κ$, representing the corresponding instantiation
objects types, regions and effects\footnote{We do not distinguish between
region variables and region constants; a region constant is simply a region
variable whose scope is the entire program.}. Now, it can be seen that
generalization takes place at \letml-bindings and instantiation at variable
occurrences. We use a horizontal bar over terms (like in ${\overline{χ}}$) to
denote lists.

\subsection{Typing}

We have already seen that programs and specification share in fact the same
syntax. Though they are differently typed, their typing relations still share
a lot. For this reason, and to underline the fact that in Hoare logic, values
are shared between programs and the logic, we present three different typing
relations. One for programs, one for specifications, and the common part is
factored out in a third typing relation for values. All typing relations
contain an environment $Γ$, which is simply a map from variable names to {\em
type schemes} of the form $∀\alist{χ}.τ$ .

\begin{figure}[tbp]
  \framebox{$Γ|-_v t : τ$}
  \begin{mathpar}
  { \inferrule*[Left=Var] {Γ(x) = ∀\alist{χ}.τ} {Γ|-_v x~[\alist{κ}] :
  τ[\alist{χ}|->\alist{κ}]} } \and
  { \inferrule*[Left=PureFun] {Γ,x : τ' |-_v t : τ} {Γ|-_v λx : τ'.t : τ' ->
  τ} } \and
  { \inferrule*[Left=Rec] 
  { Γ' = Γ, x : τ' \\ Γ'' = Γ',~f : τ' ->^ε τ \\ Γ'' |- t : τ,ε \\ Γ' |-_l
  p : \efft{ε} -> \propml \\ Γ' |-_l q : \efft{ε} ->
  \efft{ε} -> τ -> \propml } 
      {Γ|-_v \recml~f~(x : τ').\{p\}t\{q\} : τ' ->^ε τ} 
  } \and
  { \inferrule*[Left=PureApp]
    {Γ |-_v t1 : τ' -> τ \\ Γ|-_v t2 : τ'}
    {Γ |-_v  t1 ~ t2 : τ}
  }
  \end{mathpar}
  \caption{Value typing}
  \label{fig:valuetyping}
\end{figure}

\paragraph{Value typing.}
Let us first look at the common part concerning values. We define a relation
$Γ|-_v t : τ$, which means ``$t$ is well-typed wrt. the environment $Γ$, and
is of type $τ$''. Values cannot have any effect.  Variables and pure
abstractions are typed in the usual way; type instantiations at variable
occurrences are substituted in the type. The rule {\sc REC} simply states that
the body of the function can use the argument $x$ as well as the function $f$.
The specifications $p$ and $q$, however, cannot use $f$ and must be well-typed
wrt. the typing relation $|-_l$ described later on. Finally, pure applications
between values are considered as values as well, as they don't have any
effect.

\begin{figure}[tpb]
  \framebox{$Γ|- t : τ,ε$}
\begin{mathpar}
  { \inferrule*[Left=Value]
  { Γ |-_v t : τ  }
  { Γ |- t : τ,\emptyset  }
  } \and
  { \inferrule*[Left=App]
    {Γ |-_v t1 : τ' ->^ε τ \\ Γ|-_v t2 : τ'}
    {Γ |- t1 ~ t2 : τ, ε}
  } \and
  { \inferrule*[Left=ite]
    { Γ |- _v t1 : \boolml \\ Γ |- t2 : τ, ε1 \\ Γ |- t3 : τ, ε2 }
    { Γ |- \ifml~ t1 ~\thenml~ t2 ~\elseml ~ t3 : τ, ε1  ε2 }
  } \and
  { \inferrule*[Left=Letv]
    { Γ, \alist{χ} |-_v t1 : τ' \\ Γ, x : ∀\alist{χ}.τ' |- t2 : τ,ε }
    { Γ |- \letml~ x~[\alist{χ}] = t1 ~ \inml~ t2 : τ, ε}
  } \and
  { \inferrule*[Left=Let]
    { Γ |- t1 : τ', ε \\ Γ, x : τ' |- t2 : τ, ε }
    { Γ |- \letml~ x = t1 ~ \inml~ t2 : τ, ε }
  } \and
  { \inferrule*[Left=Sub]
    { Γ |- t : τ', ε \\ ε \subseteq ε' }
    { Γ |- t : τ, ε' }
  }
\end{mathpar}
  \caption{Program typing}
  \label{fig:progtyping}
\end{figure}

\paragraph{Program typing.}
The typing of programs, written $Γ|- t : τ,ε$, assigns a type $τ$ and an
effect $ε$ to an expression $t$, given an environment. This typing is
presented in {\em A-normal form}, a normal form where all intermediate values
are \letml-introduced. This is not a restriction, as any program can be
transformed an A-normal form.

All values are also programs with no effect (rule {\sc
Value}). An application must consist of two values and the effect of the
overall expression is precisely the effect of the called function (rule {\sc
App}). There are two rules for \letml, due to the value restriction. One only
can generalize type, effect and region variables when the bound expression is
a value. In this case (rule {\sc LetV}), only the second term may produce
effects. In the other case (rule {\sc Let}), no generalization takes place,
but both terms can have effects. The {\sc Let} rule actually obliges both
terms to have the same effect, but together with the {\sc Sub} rule, which can
increase the effect of an expression, this can always be achieved.
Finally, \ifml-expressions are typed in the usual way, both branches have to
have the same effect.

\paragraph{Logic typing.}

The typing of annotations is basically an extension of the typing for values;
logic terms never have any effect. To ensure this, we need to forbid calling
effectful functions in the logic. One way is to simply omit the typing rule
for effectful function calls. There is, however, another way which also
enables us to inspect the pre- and postcondition of a function. To this end,
let us introduce the operation of {\em lifting} a type to the logic level (see
Fig.~\ref{fig:typelift}).
\begin{figure}[tpb]
  \begin{eqnarray*}
    \ceil{α} &=& α\\
    \ceil{τ~\refml_ρ} &=& \ceil{τ}~\refml_ρ\\
    \ceil{τ -> τ'} &=& \ceil{τ} -> \ceil{τ'}\\
    \ceil{τ \times τ'} &=& \ceil{τ} \times \ceil{τ'}\\
    \ceil{\efft{ε}} &=& \efft{ε}\\
    \ceil{τ ->^ε τ'} &=& (\ceil{τ} -> \efft{ε} -> \propml)\times (\ceil{τ} ->
    \efft{ε} -> \efft{ε} -> \ceil{τ'} -> \propml)
  \end{eqnarray*}
  \caption{Lifting types}
  \label{fig:typelift}
\end{figure}
This translation traverses the type and replaces all effectful types by a
tuple type which describes a tuple of a pre- and a postcondition. The
precondition takes as an argument the argument of the function as well as the
part of the store, at the moment of the function call, which corresponds to
the effect of the function. The postcondition's two first arguments are the
same, but in addition it expects the store {\em at the exit of the function},
as well as the function's return value.

\begin{figure}[tbp]
  \framebox{$Γ|-_l t : τ$}
  \begin{mathpar}
  { \inferrule*[Left=Logic]
    { Γ |-_v t : τ  }
    { Γ |-_l t : \ceil{τ} }
  } \and
  { \inferrule*[Left=LetLogic]
    { Γ, \alist{χ} |-_l : t' : τ' \\ Γ, x: ∀\alist{χ}.τ' |-_l t : τ  }
    { Γ |-_l \letml~x~[\alist{χ}] = t'~\inml~t : τ }
  }\and
  { \inferrule*[Left=IteLogic]
    { Γ |- _l t1 : \boolml \\ Γ |-_l t2 : τ \\ Γ |-_l t3 : τ }
    { Γ |- \ifml~ t1 ~\thenml~ t2 ~\elseml ~ t3 : τ }
  }
  \end{mathpar}
  \caption{Logic typing}
  \label{fig:logictyping}
\end{figure}

Now we can come back to logic typing. Every value is also a logic term,
but now it is assigned its {\em lifted} type. The two other rules are just 
pure variants of their impure counterparts in the program typing.

\paragraph{Predefined constants.}

To actually be able to write programs in this language, we need many
predefined constants and types. The typing relations already mention two,
\propml\ and \boolml. \propml\ is the logical truth and contains the values
\Trueml\ and \Falseml, while \boolml\ describes boolean (decidable) properties
and contains \trueml\ and \falseml. The distinction between \propml\ and
\boolml\ is the same one as in \coq, for example. Properties expressed in
\propml\ are potentially undecidable and one cannot take decisions by testing
their truth value. On the other hand, \boolml\ talks about decidable
properties and so it makes sense to expect the predicate of the
\ifml-statement to be of type \boolml.

The reader may have wondered why the specification language does not contain
any logical connectives. These can of course be declared independently, for
example the conjunction is just a function: \begin{equation*} /\ :  \propml ->
\propml -> \propml \end{equation*} Actually, when equality is given, all
logical operations (even quantification) can be defined in the
logic~\cite{andrews86}.  In the remainder of the paper, we suppose
predefined the usual logical connectives conjunction, disjunction,
implication and equivalence, as well as universal and existential
quantification. We also assume all the arithmetic operations as well as the
constructor $(t,t)$  and the accessors $fst$ and $snd$ for pairs. We will also
use the names \preml\ and \postml\ for $fst$ and $snd$, respectively.

% TODO TODO
%Finally, we introduce two symbols
%\begin{eqnarray*}
%  \oplus &:&∀ e1 e2 . \efft{ e1 } -> \efft{ e2 } -> \efft { e1  e2 }\\
%  |_{ε} &:& ∀ e. \efft{ e ε } -> \efft{ ε }.
%  !! & : & ∀αρ e. α~\refml_ρ -> \efft{ ρ e } -> α
%\end{eqnarray*}
%The first one denotes a combining operation for stores; the resulting store
%contains all the mappings of the second store, as well as all the mappings of
%the first one, as long as they are not present in the second one.
%union of the two arguments, in the sense that its domain is the union of its
%arguments.

\subsection{References}

We haven't mentioned references yet, but all we need to support them is to add
the three usual functions $ref, :=, !$ for reference creation, assignment and
lookup, respectively, to the environment. In our system, these functions have
the following types:
\begin{eqnarray*}
  ref &:& ∀αρ.α ->^ρ α~\refml_ρ\\
  := &:& ∀αρ.α~\refml_ρ ->^\emptyset α ->^ρ \unitml\\
  ! &:& ∀αρ. α~\refml_ρ ->^ρ α
\end{eqnarray*}

\subsection{Alias restriction}

The presented programming language, along with the specifications, can, in
fact be used to prove programs. The weakest precondition calculus presented in
the next section can be used to obtain proof obligations which, when proved,
guarantee the correctness of the program with respect to its specification. On
the other hand, in its presented form, and because of the presence of
aliasing, one would constantly need equalities and inequalities between
reference variables to be able to reason about a program.

\begin{figure}[tpb]
  \begin{mathpar}
    {\inferrule*[Left=Var]
      {Γ(x) = ∀\alist{χ}.τ \\ σ = [\alist{χ}|->\alist{κ}] \\ σ \sim τ }
      {Γ |- x : τσ }
    } \\
    { \inferrule*[Left=LetRef, Right={ρ \textup{fresh}, $ρ\notin τ$}]
      {Γ |-_v t1 : τ' \\ Γ, x : τ'~\refml_ρ |- t2 : τ, ε }
      {Γ |- \letml~x = ref~ t1 ~\inml~ t2 : τ, ε \setminus ρ}
    }
  \end{mathpar}
  \caption{A modified and a new rule for aliasing}
  \label{fig:aliasing}
\end{figure}

A way out is to restrict programs to alias free programs. Fortunately, it is
easy to modify the presented typing rules to render all programs with aliasing
ill-typed (see Fig~\ref{fig:aliasing}). In the following, we will use regions
to identify memory locations entirely; each region shall describe precisely
one memory location. Starting from this assumption, a program can actually
exhibit {\em two} forms of aliasing. In the first one, it may use several
names (region variables) for the same memory location. This can happen when a
function is generalized over, say, two region variables, assuming they are
different, and is then instantiated with the same region variable twice. The
classical example is the following program
\begin{who}
  let f [ρ ρ'] (x : int ref(ρ)) (y : int ref(ρ')) =
  { }
  x := !x + 1;
  !y
  { r : r = !!y old }
\end{who}
which is only correct if $ ρ $ and $ ρ' $ are indeed different. To enforce
this, we add a side condition to the instantiation: the substitution $σ$ must
be {\em compatible} with the type $τ$. To understand what compatibility means,
consider an {\em atomic} region substitution (whose domain contains only one
variable) $[ρ|->ρ']$. It is compatible with any type which does not contain
$ρ'$. Similarly, the effect substitution $[e|-> ρ1  ρ2 e'$] is compatible with
any type which does not contain $ ρ1 , ρ2 $ nor $e'$.  Expressed more
generally, an atomic substitution is compatible with a type if the type does
not contain any region or effect variables which are in the range of the
substitution. Finally, a {\em composite} substitution (whose domain contains more
than one variable) is compatible
with a type if each atomic substitution is compatible with the result of the
application of the previous substitution. In this way, it becomes impossible
to have different names for a single reference cell.

The second form of aliasing is when we use the same name for {\em different}
memory locations. This can happen when we allow the user to call $ref$ several
times with the same region instantiation. A simple possibility is to attach
the suppress the $ref$ function entirely and to replace it by a \letml\refml\ 
construct which combines name creation and memory allocation. In this way,
there can of course be only one reference per name. On the other hand, because
of the side condition $ρ\notin τ$, the reference cannot escape the scope of
its \letml\refml\ construct, and so, a function cannot return a reference
which has been created in its body. This restriction could be leveraged a bit if
one separates name creation and allocation, while still maintaining that one
can only allocate a single reference in each region. This is how it is done in
the correctness proof of this restriction. The presented \letml\refml\
construct is of course a special case.
%TODO clearly state possibilities/limitations of the system
% * one can store everything in refs
% * but aliasing restriction (refs in refs, lists of refs etc)

%TODO now, all operations on maps can be statically resolved.

\subsection{Some properties}

%TODO
$Γ|-_v v : τ$ implies $Γ |-_l \ceil{v} : \ceil{τ}$

Subject reduction of our type system is a consequence of subject reduction of
the region calculus of Tofte/Talpin~\cite{tofte97ic}.

\section{A Simple Weakest Preconditions Calculus}
\label{sec:wp}

The aim of the weakest precondition calculus is to obtain proof obligations
from the annotated program. To do this, the calculus starts from a formula
which holds, or is supposed to hold, at the end of the program, and walks
backwards in the program source to obtain the weakest formula which has to
hold at the beginning of the program (hence the name {\em weakest
precondition}. 

In the following, we use the metasymbol $v$ to denote well-typed values, i.e.
terms with a typing derivation $Γ|-_v v : τ$. In the definition of the weakest
precondition calculus, we will use the notation $(t : τ, ε)$ to denote that
$t$ has type $τ$ and effect $ε$. We assume that applications of the
subeffecting rule {\sc Sub} rule are explicitly inserted in the term; they
are written $(t:ε<:ε')$.

\begin{figure}[tbp]
  \begin{eqnarray*}
    \ceil{x~[\alist{κ}]} &=& x~[\alist{κ}]\\
    \ceil{λx:τ.t} &=& λx:τ.\ceil{t}\\
    \ceil{\recml~f~(x:τ).\{p\} t \{q\}} &=& 
    (λx:\ceil{τ}.p,λx:\ceil{τ}.q)\\
    \ceil{t_{τ' -> τ}~t'} &=& \ceil{t}~\ceil{t'}
  \end{eqnarray*}
  \caption{Lifting values}
  \label{fig:valuelift}
\end{figure}

First of all, analogous to the lifting of types to the logic level, we define
a lifting of values to the logic level. Similarly, this translation traverses
the value until it encounters an impure function. From the logic point of
view, an impure function is represented by its specification, so the
translation keeps only the pair of pre- and postcondition, both abstracted
over the function argument. The function body is thrown away.

\begin{figure}[tbp]
  \begin{eqnarray*}
    c(x~[\alist{κ}]) &=& \Trueml \\
    c(λx:τ.t) &=& ∀x:\ceil{τ}.c(t) \\
    c(t~t') &=& c(t) /\ c(t') \\
    c(\recml~f~(x:τ).\{p\}(t : τ',ε)\{q\}) &=&
    ∀x:\ceil{τ}.∀f:\ceil{τ->^ετ'}.  f = \ceil{v} => \\& &
    ∀s:\efft{ε}.p~s => \wpre_s(t,q~s) \\
    \wpre_s(v, q) &=& c(v) /\ q~s~\ceil{v}\\
    \wpre_s( t_{τ'->^ε τ} t' ,q) &=& \correct{t} /\ \correct{t'} /\
    fst~\ceil{t}~\ceil{t'}~s /\ \\ 
    & & ∀s':\efft{ε}∀ x:\ceil{τ}.~  snd~\ceil{t}~\ceil{t'}~s~s'~x => q~s'~x\\
    \wpre_s(\letml~x~[\alist{χ}] = v~\inml~t, q) &=&
      ∀\alist{χ}.c(v) /\ \letml~x~[\alist{χ}]=\ceil{v}~\inml~ \wpre(t,q)\\
    \wpre_s(\letml~x = ( t1 : τ,ε) ~\inml~ t2 , q) &=&
    \wpre_s( t1 ,λs':\efft{ ε }.λx:\ceil{τ}.\wpre_{s'}( t2 , q ))\\
    \wpre_s(\ifml~ t1 ~\thenml~ t2 ~\elseml~ t3 , q) &=&
      \ifml~ \ceil{ t1 }~\thenml~ \wpre_s( t2 , q))~\elseml~\wpre_s( t3 , q)) \\
      \wpre_s( (t : ε <: ε'), q) &=& \wpre_{s|_ε}(t, λs':\efft{ε}.q~(s\oplus s'))
  \end{eqnarray*}
  \caption{The weakest precondition calculus}
  \label{fig:wp}
\end{figure}

The weakest precondition calculus itself comes in two parts, one for values
and one for effectful terms. Both are reasonably straightforward. The {\em
correctness} of values, written $c(v)$, ensures that any effectful function
appearing in a value adheres to its specification. To this end, the value is
traversed, quantifying over bound variables and using conjunction for pure
applications. When an effectful function is encountered, one has to prove that
the precondition of the function implies the postcondition, given the function
body.

The part concerning effectful functions, denoted $\wpre_s(t,q)$ takes as
argument a term $t$ of type $τ$ and effect $ε$, a postcondition $q$ of type
$\efft{ε} -> τ -> \propml$ and a current state $s$ of type $\efft{ε}$. If the
term is in fact a value, the value has to be correct and the postcondition
must be true for the lifted value and the current state. In the case of an
application, both values must be correct, the precondition of the function
must hold in the current state, and for any state and return value of the
function, the function's postcondition must imply the condition $q$ to be
proved.

As for the typing relation, we have two rules for the \letml-binding. If the
bound term is a value, we can translate the \letml-binding in the program by a
\letml-binding in the language. In the other case, the \wpre\ of $ t2 $ becomes
the argument of the \wpre of $ t1 $ (we denote $ε$ the effect of $ t1 $ and $
t2 $). The rule for \ifml-expressions is straightforward. The rule for
subeffecting adapts the domain of the current store and the expected domain of
the postcondition so that the types work out.

%TODO lemmas about correctness
\section{Examples}
\label{sec:examples}

In this section, we will show two simple examples which use higher-order
features.

\subsection{A simple memoization function}

\begin{figure}[tpb]
\begin{who}
type ('a,'b) map
logic mem : forall ('a, 'b) . 'a -> ('a,'b) map -> bool
logic get : forall ('a, 'b) . 'a -> ('a,'b) map -> 'b
logic set : forall ('a, 'b) . 'a -> 'b -> ('a,'b) map -> ('a,'b) map

let stores f0 m =
  forall x : Z, mem x m = true -> get x m = f0 x

parameter table : (int,int) map ref(ρ)

let memo [e] (f0 : int -> int) (f : int ->{} int) (x : int) = 
  {pre f x /\ stores f0 !!table}
  if mem x !table then get x !table
  else
    let z = f x in
    table := set x z !table;
    z
  {r : post f x r}
\end{who}
  \caption{The code along with the specification of the {\tt memo} function}
  \label{fig:memo}
\end{figure}

Our first example is a simple memoization function {\tt memo} for (pure)
functions.  It takes as an argument a function (from {\tt int} to {\tt int}
here) and an integer and returns the same result as the functional argument
would; however, before executing the argument, it will look up the integer in
a table to see if the function hasn't been called yet on this integer. If so,
the table's value is returned, otherwise we call the function, store the
result in the table, and return it. Fig~\ref{fig:memo} shows the code along
with the specification. The specification is very simple; it states that {\tt
memo} will return a value that verifies the postcondition of {\tt f}, as long
is initially we have the precondition of f and as long as the table stores
only valid key/value pairs. As we are not interested in the exact way of the
implementation of the map, we simply model it here by a reference to a
functional map with get/set functions.

\subsection{ymemo}

TODO %TODO
\begin{figure}[tpb]
\begin{who}
  let realizes 
  let ymemo (f0 : int -> int) 
            (ff : (int ->{table} int) ->{} int ->{table} int) = 
  { forall (k : [[int ->{table} int ]]),
      { realizes f0 k} ff k { r : realizes f0 r} }
  let rec f (x : int) : int ->{table} int =
    { stores f0 !!table }
    if mem x !table then get x !table
    else
      let z = ff f x in
      table := set x z !table; z
    {r : r = f0 x /\ stores f0 !!table}
  in f
  { rf : realizes f0 rf }
\end{who}
\caption{The {\tt ymemo function} and its specification}
  \label{fig:ymemo}
\end{figure}

\subsection{All examples - an overview}

A prototype implementation of the presented system by the name ``\who''
exists~\cite{KanigFilliatre09wml} and is freely available. It implements the
described language with type, region and effect polymorphism, higher-order
logic and store types in the logic. It takes as an input a fully annotated
program and outputs proof obligations which have to be proved to guarantee the
correctness of the program with respect to its specification. Currently, only
output to the interactive proof assistant Coq has been implemented. Our logic
is a subset of Coq's, so the translation is particularly simple.
The implementation consists of less than 3000 lines of \ocaml\ code, including
a limited form of type and effect inference.

\begin{figure}[tbp]
\begin{center}
    \begin{tabular}{ | r | r | r | r |}
    \hline
    Program & Ocaml Code & Code + Spec in \who & Proofs \\ \hline
    Memo/ymemo & 15 loc & 26 loc & 90 lop  \\ \hline
    {\tt Array} & 69 loc & 170 loc  & 320 lop  \\ \hline
    {\tt Hashtbl} (incomplete) & & &  \\ \hline
    Koda-Ruskey~\cite{KanigFilliatre09wml} & 10 loc & 30 loc & $\sim$700 lop  \\ \hline
    \end{tabular}
\end{center}
  \caption{A summary of all programs proved so far using the \who\ tool}
  \label{fig:whoproofs}
\end{figure}

Fig.~\ref{fig:whoproofs} shows an overview over the programs we have been able
to prove using the \who\ tool. The first line summarizes the numbers of the
memo/ymemo example of the previous subsections. 

The next two lines concern the two modules {\tt Array} and {\tt Hashtbl} of
the \ocaml\ standard library. For the {\tt Array} module, all functions
present in the \ocaml\ module have been proved, with the exception of the {\tt
Array.map} family, whose specification is very similar to {\tt iter}, and the
{\tt Array.sort} function, which is quite complicated to prove, but not very
interesting concerning higher-order functions or side effects. In this proof
development, arrays are modeled as references to functional arrays with the
usual axioms on access and update.

The proof development concerning the {\tt Hashtbl} module is ongoing work. The
difficult part of this development is the model of the \ocaml\ hash table; In
\ocaml, when one overwrites a previously existing entry in the table, the old
value still exists, and is only {\em shadowed} by the new key/value pair. If
one calls {\tt Hashtbl.remove} using this key, the old value reappears. Also,
if several bindings for a key exist, the key may be encountered several times,
during a call of {\tt Hashtbl.iter}, for example. A possible way of modeling
this is an association list instead of a simple mapping, where keys in front
of the list can shadow the same key further down in the list. 
% TODO Currently, only
%two functions of this module have been proved correct.

Finally, the last line shows the numbers for the proof development of the
Koda-Ruskey algorithm~\cite{KodaRuskey93}. A small, efficient
implementation~\cite{FilliatrePottier02} is shown to be correct. More details
about this development can be found in~\cite{KanigFilliatre09wml}.

\section{Related Work}
\label{sec:related}

Honda, Yoshida and
Berger~\cite{Honda05anobservationally,BergerHondaYoshida05aliasing} present a
logic for imperative higher-order programs. They present their calculus using
rules in Hoare logic style, while we presented a \wpre-calculus. Their system
does not provide the same modularity than ours, as one can not reason
modularly about side effects of functional arguments. It also has the drawback
of being difficult to implement, due to the presence of a large number of
non-structural rules. Our system, starting from a correctly annotated program,
is syntax-directed and thus entirely automated. Their logic of proof
obligations is non-standard, while we obtain obligations in higher-order
logic. They deal with aliasing using equalities and disequalities between
pointers, which we chose not to do. They can reason about the classic {\em
Landin's knot}, which consists in obtaining recursion by storing a function in
a reference which calls this same reference. Our typing restriction prohibits
this particular kind of programs.  Honda, Yoshida and Berger prove a
completeness result for their system. We have proved no such result.

%, using a slightly more
%expressive language than ours. However, their calculus can
%deal easily with functions in references, even if the stored function
%reads or modifies the reference it is stored in. While, with the
%appropriate extensions, one can reason about functions in references
%in our system, this particular kind of programs cannot be reasoned
%about.  Their extended
%system~\cite{BergerHondaYoshida05aliasing} has the same advantages and
%drawbacks compared to ours, but additionally deals with aliasing
%situations.

The Coq proof assistant~\cite{CoqManualV81}, in particular in combination with
the Program extension~\cite{sozeau07icfp}, can be used to implement and prove
arbitrary properties of purely functional programs, using dependent types. The
appeal of that approach is that the only limit of expressiveness is the one of
Coq. One can also do meta-reasoning about programs inside Coq and, at the end
of the development, extract a certified implementation in \ocaml\ or Haskell.
The confidence in programs proved in this way is very high, because in the end
one obtains a Coq proof term that corresponds to the correctness of the
program. This term has been checked by Coq's kernel. However, without
extensions such as Ynot (see next paragraph), it is impossible to implement
(and reason about) effectful computations. Another drawback of this approach
is lack of automation.

The Ynot System~\cite{Nanevski08Awkward,chlipalaicfp09} is an extension to the
Coq proof assistant, capable of reasoning about imperative higher-order
programs, including effectful functions as arguments, using a monad in which
effectful computations can take place. It uses higher-order separation logic
as the device to speak about modifications of the state. Ynot uses an
(arguably less intuitive) monadic representation of effectful computations.
Ynot is more expressive than our system because it uses Coq as the underlying
system, and features modular reasoning (abstracting over the specification and
effect of a function as argument) while being able to reason about aliasing
situations thanks to separation logic. Ynot also has limited support for
proof automation, thanks to tactics dealing with separation logic
formulas~\cite{chlipalaicfp09}. We claim that we can obtain much better
automation for alias-free programs, and even higher-order functions. We
explain in the next section how we plan to achieve this.

%On the other hand, Ynot shares the
%Coq's main weakness: lack of automation. While we haven't yet presented a
%platform to automatically discharge generated proof obligations, this seems
%easily feasible, along the same vein as Régis-Gianas and
%Pottier~\cite{regis-gianas-pottier-08}, who argue that first-order SMT
%solvers~\cite{RanTin-SMTLIB} are suitable even for higher-order proof
%obligations. They also present an encoding scheme to support their claim.
%Their practical results are promising. The Ynot system can not use this kind
%of automation because it uses separation logic.  Automated provers for
%separation logic exist, but we are not aware of any attempts to use these
%tools in a higher-order setting.

%TODO also say that an approach like Ynot ou Coq doesn't give any help
%for the mechanical part of the proof.

The Pangolin system, the implementation of the theoretical system
of~\cite{regis-gianas-pottier-08}, can also be used to reason about
purely functional programs. It is also one of the starting points of
our work, and our purely functional fragment is basically the Pangolin
system, although we removed algebraic data types for clarity of
presentation. For this reason, we share the potential advantages (good
automation, relatively simple system) and drawbacks (no
meta-reasoning or observational equality, no machine-checked proof
term). Pangolin cannot deal with side effects.

Another line of work consists in giving a translation from an imperative to a
functional language, with the aim of applying one of the techniques for purely
functional programs to the translation. O'Hearn and
Reynolds~\cite{ohearn-reynolds-00} present a translation from Algol with first
class references and dynamic allocation to a linear $\lambda$-calculus. They
impose an alias-avoiding restriction which is similar to ours. Charguéraud and
Pottier~\cite{chargueraud-pottier-08} give a translation from Core ML with
first-class references, dynamic allocation and aliasing to a functional
language, using a very expressive type system with linear capabilities and
group regions, while we only accept singleton regions. The second system can
deal with aliasing programs. However, none of these works describe a mechanism
to obtain proof obligations, for example a \wpre-calculus.  We present a
complete cycle from imperative programs to proof obligations. We also claim
that extending our language and our \wpre-calculus is simple, as long as
aliasing is tracked precisely.  We leave these extensions for future work.

Systems for verification of first-order programs like
Why~\cite{Filliatre00a} or Spec\#~\cite{BarnettLS04} are the other
starting points of this work. These systems generally deal very well
with usual features of first-order programs, for example arrays, use
SMT solvers to discharge proof obligations and strive for the best
possible automation. We try to improve on this work by adding
higher-order features, trying to maintain the degree of automation and
ease of use.

\section{Conclusion}
\label{sec:conclusion}

We have presented a Hoare logic for higher-order programs with side
effects. Its main feature is the ability to reason about higher-order
functions in a {\em modular} way. A higher-order function can be
specified by abstracting over the specification and the effect of its
arguments. This specification can be reused when the higher-order
function is instantiated with a particular effect and applied to a
particular argument. This modularity is achieved by combining two
previously known mechanisms. Effect polymorphism enables us to
abstract over the modifications of a function. Reflecting values and
types in a higher-order logic enables us to abstract over the
specification of a function. We have proposed a way to
reason about the evolution of a store, with potentially unknown
components represented by effect variables. We have limited ourselves to
programs without aliasing.

In its simplicity and its drawbacks, our system is similar to the one
of ~\cite{regis-gianas-pottier-08}. The system's definition holds
entirely in Figure~\ref{fig:wp}, and it can be easily
implemented because it is syntax-directed. On the other hand, we only
consider partial correctness. Also, we can not reason about
observational equivalence of programs. In our system, two functions
are equivalent if their specifications are.


\paragraph{Future Work.}

We are currently investigating the possibility to obtain first-order proof
obligations, to be able to use SMT solvers~\cite{RanTin-SMTLIB}, which would
avoid manual proofs in many cases. An obvious solution would be to reuse
existing encodings of higher-order logic in first-order logic, such as the one
by Pottier and Gauthier~\cite{pottier-gauthier-hosc}. This technique has been
successfully employed by the Pangolin system~\cite{regis-gianas-pottier-08}.
Another option comes from our observation that in many cases, the proof
obligations for higher-order functions can actually be expressed in
first-order logic, when one replaces top-level {\em quantifications} of
functional arguments and invariants by top-level {\em declarations}. Doing
this translation manually, we have obtained first-order proof
obligations for the {\tt Array.iter} function which are all discharged by
SMT solvers. It is future work to find out if such a transformation can always
be carried out.

Currently, our effect system is quite limited. One possible extension is the
addition of subtyping: an effectful function $f$ of type $τ ->^ετ'$ can be seen
as a function of type $τ ->^{ε'}τ'$, if $ε\subseteq ε'$. In particular, such a
subtyping operation should come at no cost for program verification; it should
still come for free that $f$ does not modify the variables in $ε'\setminus ε$. It
would also be interesting to see if an ``impure'' function of type $τ
->^\emptyset τ'$ can be seen as pure function $τ -> τ'$ (and be used in the
logic).

We see two possibilities to add the possibility to reason about aliasing
programs. The first one is to modify the effect system itself to allow for
aliasing. In the setting with the aliasing restriction, our regions actually
become {\em singleton regions}, regions containing exactly one memory
location. Other, more powerful effect systems allow for {\em group regions},
with the possibility to temporarily {\em focus } on a particular memory cell
in that region. We see no fundamental difficulty in adopting such an effect
system, but haven't worked out the details. A second possibility is to deal
with aliasing via equalities and disequalities over pointers, using regions to
obtain some separation automatically. On itself, we believe that this
technique does not scale, due to the explosion of needed disequalities.
However, in combination with other separation techniques, for example the
Burstall-Bornat memory model which separates automatically objects of
different structural types, this approach has worked well applied to
first-order programs.

Finally, from a programmer's point of view, it would be nice to have more
features in the programming language, such as recursive types and algebraic
data types. As long as so defined types verify a {\em positivity
condition}~\cite{paulintlca93}, these extensions would be completely
orthogonal to the weakest precondition calculus and the type and effect system
used.


\bibliographystyle{plain}
\bibliography{biblio}{}

\end{document}
